{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import yaml\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from contextlib import closing\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(  \n",
    "        config_filename: str,\n",
    "        path_to_dir: str = None) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Read a YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_filename (str): The name of the configuration file.\n",
    "        path_to_dir (str, optional): The path to the directory containing the configuration file.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: The configuration data as a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    path_to_configfile = os.path.join(\n",
    "        path_to_dir, \n",
    "        config_filename) \\\n",
    "            if path_to_dir is not None else config_filename\n",
    "    \n",
    "    with open(path_to_configfile) as yaml_file:\n",
    "        config = yaml.load(yaml_file, Loader=yaml.SafeLoader)\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION = read_config(config_filename='CONFIG.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SITE_URL = CONFIGURATION.get('BASE_SITE_URL')\n",
    "RESULTS_ENDPOINT = CONFIGURATION.get('RESULTS_ENDPOINT')\n",
    "ZIP_FILE_PATH = CONFIGURATION.get('ZIP_FILE_PATH')\n",
    "DATA_DIR = CONFIGURATION.get('DATA_DIR')\n",
    "EXTRACTED_DATA_DIR = CONFIGURATION.get('EXTRACTED_DATA_DIR')\n",
    "NEEDED_TABLES = CONFIGURATION.get('NEEDED_TABLES')\n",
    "EXT_NAME = CONFIGURATION.get('EXT_NAME')\n",
    "USED_VERSION_FILE = CONFIGURATION.get('USED_VERSION_FILE')\n",
    "DB_NAME = CONFIGURATION.get('DB_NAME')\n",
    "CREATES_SQL_FILE = CONFIGURATION.get('CREATES_SQL_FILE')\n",
    "NUMBER_OF_ROWS_FILE = CONFIGURATION.get('NUMBER_OF_ROWS_FILE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_destination_download_endpoint(\n",
    "    base_site_url: str,\n",
    "    results_endpoint: str) -> str:\n",
    "\n",
    "    response = requests.get(f\"{base_site_url}{results_endpoint}\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    link_elements = soup.find_all('a')\n",
    "\n",
    "    destination_download_enpoint = [\n",
    "        link.get('href') for link in link_elements \n",
    "        if str(link.get('href')).endswith('Z.tsv.zip')][0]\n",
    "\n",
    "    return f\"{base_site_url}{destination_download_enpoint}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_download_endpoint = get_destination_download_endpoint(\n",
    "    base_site_url=BASE_SITE_URL,\n",
    "    results_endpoint=RESULTS_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_version(\n",
    "    used_version_path: str,\n",
    "    file_to_download_currently: str) -> bool:\n",
    "\n",
    "    web_version = file_to_download_currently.split(\".\")[0]\n",
    "\n",
    "    with open(used_version_path, 'r', encoding='utf-8') as used_version_json:\n",
    "        used_version_dict = json.load(used_version_json)\n",
    "\n",
    "    if not used_version_dict:\n",
    "\n",
    "        with open(used_version_path, 'w', encoding='utf-8') as used_version_json:\n",
    "            json.dump({'used_version': web_version}, used_version_json)\n",
    "\n",
    "        return True\n",
    "\n",
    "    used_version = list(used_version_dict.values())[0].split(\".\")[0]\n",
    "    \n",
    "    if used_version != web_version:\n",
    "\n",
    "        with open(used_version_path, 'w', encoding='utf-8') as used_version_json:\n",
    "            json.dump({'used_version': web_version}, used_version_json)\n",
    "\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.basename(destination_download_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "if not compare_version(os.path.join(DATA_DIR, USED_VERSION_FILE), file_name):\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(destination_download_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.status_code == requests.codes.ok:\n",
    "\n",
    "    save_path = os.path.join(os.getcwd(), DOWNLOADED_FILES_DIR, file_name)\n",
    "\n",
    "    with open(save_path, 'wb') as file_:\n",
    "        file_.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(os.getcwd(), DOWNLOADED_FILES_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_table(\n",
    "    filename: str,\n",
    "    needed_tables: list) -> bool:\n",
    "\n",
    "    for table_name in needed_tables:\n",
    "\n",
    "        if filename.endswith(table_name):\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_ in os.listdir(DOWNLOADED_FILES_DIR):\n",
    "\n",
    "    filename, extension = os.path.splitext(file_)\n",
    "    \n",
    "    if extension == EXT_NAME and not valid_table(filename, needed_tables=NEEDED_TABLES.keys()):\n",
    "\n",
    "        try:\n",
    "            os.remove(os.path.join(DOWNLOADED_FILES_DIR, file_))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_rows(\n",
    "    path_to_tsv_file: str) -> int:\n",
    "\n",
    "    with open(path_to_tsv_file, \"rbU\") as f:\n",
    "        num_lines = sum(1 for _ in f)\n",
    "\n",
    "    return num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(\n",
    "        DATA_DIR, \n",
    "        EXTRACTED_DATA_DIR, \n",
    "        'last_number_of_rows.json'), 'w', encoding='utf-8') as lnor_json:\n",
    "    \n",
    "    tsv_files = [file_ for file_ in os.listdir(os.path.join(DATA_DIR, EXTRACTED_DATA_DIR)) if file_.endswith('.tsv')]\n",
    "    n_dict = {f: 0 for f in tsv_files}\n",
    "\n",
    "    json.dump(n_dict, lnor_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_table_pipeline(\n",
    "        table_name: str,\n",
    "        data_dir: str,\n",
    "        extracted_dir: str,\n",
    "        needed_columns: list,\n",
    "        last_number_of_rows: int,\n",
    "        sep: str = '\\t',\n",
    "        table_prefix: str = 'WCA_export_',\n",
    "        file_ext: str = '.tsv') -> pd.DataFrame:\n",
    "    \n",
    "    path_to_tsv_file = os.path.join(\n",
    "        data_dir, \n",
    "        extracted_dir,\n",
    "        f'{table_prefix}{table_name}{file_ext}')\n",
    "    \n",
    "    new_number_of_rows = get_number_of_rows(\n",
    "        path_to_tsv_file=path_to_tsv_file\n",
    "    )\n",
    "\n",
    "    if last_number_of_rows == new_number_of_rows:\n",
    "        return None\n",
    "    \n",
    "    difference_between_versions = new_number_of_rows - last_number_of_rows\n",
    "\n",
    "    if difference_between_versions < 0:\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "            path_to_tsv_file, \n",
    "            skiprows=range(1, last_number_of_rows),\n",
    "            sep=sep,\n",
    "            usecols=needed_columns)\n",
    "\n",
    "    return table_name, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competitions_data_preparation(\n",
    "    df: pd.DataFrame,\n",
    "    mapping_columns: dict = {\n",
    "        'id': 'Id',\n",
    "        'name': 'Name'\n",
    "    }) -> pd.DataFrame:\n",
    "\n",
    "    result = (\n",
    "        df.copy()\n",
    "        .rename(columns=mapping_columns)\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puzzle_data_preparation(\n",
    "    df: pd.DataFrame,\n",
    "    mapping_columns: dict = {\n",
    "        'id': 'Id',\n",
    "        'name': 'Name'}) -> pd.DataFrame:\n",
    "\n",
    "    result = (\n",
    "        df.copy()\n",
    "        .rename(columns=mapping_columns)\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_data_preparation(\n",
    "    df: pd.DataFrame,\n",
    "    mapping_columns: dict = {\n",
    "        'day': 'Date_day',\n",
    "        'month': 'Date_month',\n",
    "        'year': 'Date_year'}) -> pd.DataFrame:\n",
    "    \n",
    "    columns_to_drop = [col for col in df.columns if col not in list(mapping_columns.keys())]\n",
    "\n",
    "    result = \\\n",
    "        df.copy() \\\n",
    "        .drop(columns=columns_to_drop) \\\n",
    "        .rename(columns=mapping_columns) \\\n",
    "        .dropna() \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index() \\\n",
    "        .astype(np.int32)\n",
    "    \n",
    "    result['Id'] = result['Date_day'].astype(str) + '_' + result['Date_month'].astype(str) + '_' + result['Date_year'].astype(str)\n",
    "    result = result.reindex(columns=['Id'] + list(mapping_columns.values()))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id(input_string):\n",
    "\n",
    "    id_string = re.sub(r' ', '_', input_string)\n",
    "    id_string = re.sub(r'[^\\w\\s-]', '', id_string)\n",
    "\n",
    "    return id_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_data_preparation(\n",
    "    competitions_df: pd.DataFrame,\n",
    "    countries_df: pd.DataFrame,\n",
    "    continents_df: pd.DataFrame,\n",
    "    mapping_columns: dict = {'cityName': 'City'},\n",
    "    required_columns: list = ['City', 'Country', 'Continent']) -> pd.DataFrame:\n",
    "\n",
    "    competitions_df = competitions_df.copy().rename(columns={'id': 'competitions_id'})\n",
    "    countries_df = countries_df.copy().rename(columns={'id': 'country_id', 'name': 'Country'})\n",
    "    continents_df = continents_df.copy().rename(columns={'id': 'continents_id', 'name': 'Continent'})\n",
    "\n",
    "    result_df = (competitions_df \n",
    "        .merge(\n",
    "            countries_df,\n",
    "            how='left',\n",
    "            left_on='countryId',\n",
    "            right_on='country_id') \n",
    "        .merge(\n",
    "            continents_df,\n",
    "            how='left',\n",
    "            left_on='continentId',\n",
    "            right_on='continents_id')\n",
    "        .rename(columns=mapping_columns)\n",
    "        )\n",
    "    \n",
    "    result_df = (result_df\n",
    "        .drop(columns=[\n",
    "            col for col in result_df.columns if col not in required_columns])\n",
    "        .dropna()   \n",
    "        .drop_duplicates()\n",
    "        .reset_index())\n",
    "    \n",
    "    result_df['Id'] = result_df.apply(\n",
    "        lambda row: generate_id(f\"{row['City'].lower()}_{row['Country'].lower()}\"), axis=1)\n",
    "    \n",
    "    result_df = result_df.reindex(columns=['Id'] + required_columns)\n",
    "  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitions_df = pd.read_csv(\n",
    "    os.path.join(DOWNLOADED_FILES_DIR, 'WCA_export_Competitions.tsv'),\n",
    "    sep='\\t',\n",
    "    usecols=NEEDED_TABLES.get('Competitions')\n",
    ")\n",
    "countries_df = pd.read_csv(\n",
    "    os.path.join(DOWNLOADED_FILES_DIR, 'WCA_export_Countries.tsv'),\n",
    "    sep='\\t',\n",
    "    usecols=NEEDED_TABLES.get('Countries')\n",
    ")\n",
    "\n",
    "continents_df = pd.read_csv(\n",
    "    os.path.join(DOWNLOADED_FILES_DIR, 'WCA_export_Continents.tsv'),\n",
    "    sep='\\t',\n",
    "    usecols=NEEDED_TABLES.get('Continents')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_df = localization_data_preparation(\n",
    "    competitions_df=competitions_df,\n",
    "    countries_df=countries_df,\n",
    "    continents_df=continents_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = time_data_preparation(\n",
    "    df=pd.read_csv(\n",
    "        os.path.join(DOWNLOADED_FILES_DIR, 'WCA_export_Competitions.tsv'),\n",
    "        sep='\\t', usecols=NEEDED_TABLES.get('Competitions')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nationality_data_preparation(\n",
    "    countries_df: pd.DataFrame,\n",
    "    continents_df: pd.DataFrame,\n",
    "    nationalities_dict: dict,\n",
    "    required_columns: list = ['Name', 'Continent']) -> pd.DataFrame:\n",
    "\n",
    "    countries_df_copy = countries_df.copy().rename(columns={'id': 'country_id', 'name': 'Name'})\n",
    "    continents_df_copy = continents_df.copy().rename(columns={'id': 'continents_id', 'name': 'Continent'})\n",
    "\n",
    "    result_df = (countries_df_copy \n",
    "        .merge(\n",
    "            continents_df_copy,\n",
    "            how='left',\n",
    "            left_on='continentId',\n",
    "            right_on='continents_id')\n",
    "        )\n",
    "    \n",
    "    result_df = (result_df\n",
    "        .drop(columns=[\n",
    "            col for col in result_df.columns if col not in required_columns])\n",
    "        .dropna()   \n",
    "        )\n",
    "    \n",
    "    result_df['Continent'] = np.where(\n",
    "        result_df['Continent'] != 'Multiple Continents', \n",
    "        result_df['Continent'] + 'n', \n",
    "        'Multiple')\n",
    "    \n",
    "    result_df['Id'] = result_df['Name'].copy()\n",
    "    result_df['Name'] = result_df['Id'].apply(lambda x: nationalities_dict.get(x))\n",
    "    \n",
    "    result_df = result_df.reindex(columns=['Id'] + required_columns)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nationality_data_preparation(\n",
    "    countries_df=countries_df,\n",
    "    continents_df=continents_df,\n",
    "    nationalities_dict=json.load(open('nationalities.json', 'r'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attendance_data_preparation(\n",
    "    results_df: pd.DataFrame,\n",
    "    competitions_df: pd.DataFrame,\n",
    "    mapping_columns: dict = {\n",
    "        'eventId': 'Puzzle_id',\n",
    "        'competitionId': 'Competition_id',\n",
    "        'personCountryId': 'Nationality_id',\n",
    "        'countryId': 'Country_id'},\n",
    "    required_columns: list = [\n",
    "        'Competition_id',\n",
    "        'Localization_id',\n",
    "        'Puzzle_id', \n",
    "        'Country_id',\n",
    "        'Time_id']) -> pd.DataFrame:\n",
    "\n",
    "    attendance_df = (results_df.copy()\n",
    "        .merge(\n",
    "            competitions_df.copy(),\n",
    "            how='left',\n",
    "            left_on='competitionId',\n",
    "            right_on='id')\n",
    "        .rename(columns=mapping_columns))\n",
    "    \n",
    "    attendance_df['Time_id'] = \\\n",
    "        attendance_df['day'].astype(str) \\\n",
    "        + '_' \\\n",
    "        + attendance_df['month'].astype(str) \\\n",
    "        + '_' \\\n",
    "        + attendance_df['year'].astype(str)\n",
    "    \n",
    "    attendance_df['Localization_id'] = attendance_df.apply(\n",
    "        lambda row: generate_id(f\"{row['cityName'].lower()}_{row['Country_id'].lower()}\"), axis=1)\n",
    "    \n",
    "    attendance_df = (attendance_df\n",
    "        .drop(columns=[\n",
    "            col for col in attendance_df.columns if col not in required_columns])\n",
    "        .dropna()   \n",
    "        .reset_index(drop=True))\n",
    "    \n",
    "    attendance_df['Number_of_participants'] = \\\n",
    "        attendance_df.groupby(list(attendance_df.columns)).transform('size')\n",
    "    \n",
    "    attendance_df = (attendance_df\n",
    "        .reindex(columns=['Number_of_participants'] + required_columns)\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True))\n",
    "  \n",
    "    return attendance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\n",
    "    os.path.join(DOWNLOADED_FILES_DIR, 'WCA_export_Results.tsv'), \n",
    "    skiprows=range(1, 3_000_000),\n",
    "    sep='\\t',\n",
    "    usecols=NEEDED_TABLES.get('Results'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CREATES_SQL_FILE, 'r', encoding='utf-8') as creates_sql:\n",
    "\n",
    "    sql_file_content = creates_sql.read()\n",
    "    sql_statements = sql_file_content.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with closing(conn.cursor()) as cur:\n",
    "\n",
    "    for create_statement in sql_statements:\n",
    "\n",
    "        cur.execute(create_statement)\n",
    "        conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
